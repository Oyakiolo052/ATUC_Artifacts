{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c91a071b-6ec2-4ec8-8b73-cd2ac81b749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    print(p)\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9135a152-c050-46bc-9b70-80342cdb39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data_path = 'detox/emnlp2021/data/test/test_10k_toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d89a23d7-d41e-4058-85d1-52dfe88e944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fd8411d-34cb-4d2e-b4ab-75ccca82f15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u1/mdr614/FSE_majorRevision/detox-20240127T202801Z-001/detox/emnlp2021/style_transfer/paraGeDi\n"
     ]
    }
   ],
   "source": [
    "add_sys_path('detox/emnlp2021/style_transfer/paraGeDi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a7fbd07-fc94-4471-9655-6be4925d1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import gedi_adapter\n",
    "reload(gedi_adapter)\n",
    "from gedi_adapter import GediAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e4aabea-8c8e-4501-81e7-559c669d1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "t5name = 's-nlp/t5-paraphrase-paws-msrp-opinosis-paranmt'\n",
    "model_path = 's-nlp/gpt2-base-gedi-detoxification'\n",
    "clf_name = 's-nlp/roberta_toxicity_classifier_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4aa6dd15-48c1-4bb9-ad42-1e24c75c860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_processing\n",
    "reload(text_processing);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "960cc697-6871-4c03-a8dc-b12607d5b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af68aed9-1b69-47f5-b4bc-fe4fe296d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(t5name)\n",
    "para = AutoModelForSeq2SeqLM.from_pretrained(t5name)\n",
    "para.resize_token_embeddings(len(tokenizer))\n",
    "para.to(device);\n",
    "para.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2981f5cd-7de7-43c5-8011-88a07b0abf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/gpt2-base-gedi-detoxification were not used when initializing GPT2LMHeadModel: ['bias', 'logit_scale']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gedi_dis = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01c8f4c9-6930-4c6c-b6fa-1b600dedd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_POS = tokenizer.encode('normal', add_special_tokens=False)[0]\n",
    "NEW_NEG = tokenizer.encode('toxic', add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd9eb12a-7dde-4459-8218-8b7846b20481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0844, -0.0844]]) tensor([[1.2702]])\n"
     ]
    }
   ],
   "source": [
    "# add gedi-specific parameters\n",
    "if os.path.exists(model_path):\n",
    "    w = torch.load(model_path + '/pytorch_model.bin', map_location='cpu')\n",
    "    gedi_dis.bias = w['bias']\n",
    "    gedi_dis.logit_scale = w['logit_scale']\n",
    "    del w\n",
    "else:\n",
    "    gedi_dis.bias = torch.tensor([[ 0.08441592, -0.08441573]])\n",
    "    gedi_dis.logit_scale = torch.tensor([[1.2701858]])\n",
    "print(gedi_dis.bias, gedi_dis.logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff66d63c-df50-4400-bd1f-f7954c432634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====BEFORE====\n",
      "The internal policy of Trump is flawed.\n",
      "====AFTER=====\n",
      "the trump has an ugly internal policy.\n",
      "the president's current and existent domestic policy is corrupt\n",
      "he is a fool.\n"
     ]
    }
   ],
   "source": [
    "dadapter = GediAdapter(model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=5, target=1, neg_code=NEW_NEG, pos_code=NEW_POS, lb=None, ub=None)\n",
    "text = 'The internal policy of Trump is flawed.'\n",
    "print('====BEFORE====')\n",
    "print(text)\n",
    "print('====AFTER=====')\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to(para.device)\n",
    "result = dadapter.generate(inputs, do_sample=True, num_return_sequences=3, temperature=1.0, repetition_penalty=3.0, num_beams=1)\n",
    "for r in result:\n",
    "    print(tokenizer.decode(r, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf3064ce-f36b-472c-b9cb-b4a1b7b66801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available() and device.type != 'cpu':\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40c6cb6b-0bd9-4e0d-a46a-eea775ed573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gedi_dis.to(device);\n",
    "gedi_dis.bias = gedi_dis.bias.to(device)\n",
    "gedi_dis.logit_scale = gedi_dis.logit_scale.to(device)\n",
    "gedi_dis.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "551f6f5f-99a9-494f-85e6-7b7ce35572ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_path = '/u1/mdr614/ATUC_Final/Conversion_uncivil_to_civil_random_2000/Random_2000_uncivil_comments_for_conversion/civilAlternatives_by_random2000.csv'  # Replace with your actual CSV file path\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Access the 'uncivil' column and convert it to a list\n",
    "test_data = df['uncivil'].tolist()\n",
    "\n",
    "# Print the number of sentences\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f3c5867-b136-4f7e-a883-15e3c9e12fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dadapter = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer, gedi_logit_coef=10, target=0, neg_code=NEW_NEG, pos_code=NEW_POS,\n",
    "    reg_alpha=3e-5, ub=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16290a18-c422-4882-a443-f1572d364aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, n=None, max_length='auto', beams=2):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(dadapter.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    result = dadapter.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False, temperature=0.0, repetition_penalty=3.0, max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        num_beams=beams,\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "    texts = [text_processing.text_postprocess(t) for t in texts]\n",
    "    if not n and isinstance(text, str):\n",
    "        return texts[0]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "515f4279-1fa2-4d6f-b5c7-74944f5869c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eapBYteBuffer[pos=0 lim=0 cap=0]] output: ByteBufferWrapper (884587465)',\n",
       " \"it looks like this will result in a fork. That's a real shame, but that's what happens if you throw the community out of the boat.\",\n",
       " 'The lack of consistency in my own ecosystem, however, is a nightmare.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(test_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5856ca71-fa31-4bcf-baea-cbbbd62a9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device);\n",
    "clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44486a43-9eff-440e-82f5-fba36b86c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(texts):\n",
    "    with torch.inference_mode():\n",
    "        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n",
    "        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1248fb62-17da-458f-aea9-646ea4729085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Fick 'Emmy.\",\n",
       " \"You'd be wrong!\",\n",
       " \"You'll remind me of chump?\",\n",
       " 'Must be a Terrorist!']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gedi_adapter import GediAdapter\n",
    "\n",
    "adapter2 = GediAdapter(\n",
    "    model=para, gedi_model=gedi_dis, tokenizer=tokenizer,\n",
    "    gedi_logit_coef=10,\n",
    "    target=0, pos_code=NEW_POS,\n",
    "    neg_code=NEW_NEG,\n",
    "    reg_alpha=3e-5,\n",
    "    ub=0.01,\n",
    "    untouchable_tokens=[0, 1],\n",
    ")\n",
    "\n",
    "\n",
    "def paraphrase(text, max_length='auto', beams=5, rerank=True):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    texts = [text_processing.text_preprocess(t) for t in texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(adapter2.device)\n",
    "    if max_length == 'auto':\n",
    "        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)\n",
    "    attempts = beams\n",
    "    out = adapter2.generate(\n",
    "        inputs,\n",
    "        num_beams=beams,\n",
    "        num_return_sequences=attempts,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=3.0,\n",
    "        max_length=max_length,\n",
    "        bad_words_ids=[[2]],  # unk\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    results = [tokenizer.decode(r, skip_special_tokens=True) for r in out.sequences]\n",
    "\n",
    "    if rerank:\n",
    "        scores = predict_toxicity(results)\n",
    "\n",
    "    results = [text_processing.text_postprocess(t) for t in results]\n",
    "    out_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        if rerank:\n",
    "            idx = scores[(i*attempts):((i+1)*attempts)].argmin()\n",
    "        else:\n",
    "            idx = 0\n",
    "        out_texts.append(results[i*attempts+idx])\n",
    "    return out_texts\n",
    "\n",
    "torch.manual_seed(0)\n",
    "paraphrase(['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .'], beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1752d976-2fc4-45db-bb01-032ca2cfcae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a47cfc1d-4280-4f19-b550-30d6aedf593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c35694b60864fe89878f33ac227e67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in /u1/mdr614/ATUC_Final/Conversion_uncivil_to_civil_random_2000/Random_2000_uncivil_comments_for_conversion/civilAlternatives_by_Paragedi_random2000.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cleanup()\n",
    "\n",
    "lines = test_data\n",
    "output_csv_path = '/u1/mdr614/ATUC_Final/Conversion_uncivil_to_civil_random_2000/Random_2000_uncivil_comments_for_conversion/civilAlternatives_by_Paragedi_random2000.csv'\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open(output_csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    for i in trange(int(len(lines) / batch_size + 1)):\n",
    "        if i % 10 == 0:\n",
    "            cleanup()\n",
    "        t = i * batch_size\n",
    "        batch = [line.strip() for line in lines[t:(t+batch_size)]]\n",
    "        if not batch:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            res = paraphrase(batch, max_length='auto', beams=10)\n",
    "        except RuntimeError:\n",
    "            print('runtime error for batch ', i)\n",
    "            try:\n",
    "                cleanup()\n",
    "                res = [paraphrase([text], max_length='auto', beams=3)[0] for text in batch]\n",
    "            except RuntimeError:\n",
    "                print('runtime error for batch ', i, 'even with batch size 1')\n",
    "                res = batch\n",
    "                cleanup()\n",
    "\n",
    "        for original, out in zip(batch, res):\n",
    "            # Write the original and paraphrased sentences to the CSV file\n",
    "            csv_writer.writerow([original, out])\n",
    "\n",
    "print(f\"Results saved in {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb9819-0a23-42e0-8bde-8bb030639575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
