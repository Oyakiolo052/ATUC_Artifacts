{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb25bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataloader_test):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in dataloader_test:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels':         batch[2],\n",
    "                }  \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        logits = outputs[1]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(df):\n",
    "    \n",
    "    encoded_test_val = tokenizer.batch_encode_plus(\n",
    "     df.message.values, \n",
    "     add_special_tokens=True, \n",
    "     return_attention_mask=True, \n",
    "     pad_to_max_length=True, \n",
    "     max_length=512, \n",
    "     return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_test = encoded_test_val['input_ids']\n",
    "    attention_masks_test = encoded_test_val['attention_mask']\n",
    "    labels_test = torch.tensor(df.label.values)\n",
    "\n",
    "    dataset_test = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "    batch_size = 8\n",
    "    dataloader_test = DataLoader(dataset_test, \n",
    "                                    sampler=SequentialSampler(dataset_test), \n",
    "                                    batch_size=batch_size)\n",
    "\n",
    "\n",
    "    pred_test = test_model(dataloader_test) \n",
    "    preds_flat_test = np.argmax(pred_test, axis=1).flatten()\n",
    "    return preds_flat_test\n",
    "#     #print(preds_flat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a40ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bart_large(c):\n",
    "    uncivil_sentence = c\n",
    "    inputs = tokenizer_Bart_large(uncivil_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    generated_ids = fine_tuned_bart_large.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_Bart_large.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_civil_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bart_base(c):\n",
    "    uncivil_sentence = c\n",
    "    inputs = tokenizer_Bart_base(uncivil_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    generated_ids = fine_tuned_bart_base.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_Bart_base.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_civil_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set the logging level for the transformers module\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Suppress specific FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `pad_to_max_length` argument is deprecated*\")\n",
    "\n",
    "# Initialize an empty list to store comments\n",
    "comments = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the fine-tuned BART_large model\n",
    "fine_tuned_bart_large = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart_large\")\n",
    "tokenizer_Bart_large = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Load the fine-tuned BART_large model\n",
    "fine_tuned_bart_base = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart_base\")\n",
    "tokenizer_Bart_base = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "\n",
    "\n",
    "input_csv_file = \"SentiStrength_SE.csv\"  # Replace with your CSV file path\n",
    "comments_df = pd.read_csv(input_csv_file, header=None, names=[\"comment\"])\n",
    "\n",
    "# Create an empty list to store comments and their associated outputs\n",
    "comments_with_outputs = []\n",
    "\n",
    "\n",
    "for inp_sentence in comments_df[\"comment\"]:\n",
    "    # Collect comment from the user or exit the loop\n",
    "    comment = inp_sentence\n",
    "\n",
    "    \n",
    "    # Add the comment to the list\n",
    "    comments.append(comment)\n",
    "    \n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "    decoded_output = t5_large(comment)\n",
    "    # Add the comment to the list\n",
    "    comments.append(decoded_output)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "\n",
    "    generated_civil_sentence = Bart_large(comment)\n",
    "            # Add the comment to the list\n",
    "    comments.append(generated_civil_sentence)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "    if preds_flat_test[-1]==0:\n",
    "        print(\"\\033[92mGenerated civil alternative-BART_large:\\033[0m\", generated_civil_sentence)\n",
    "        comments_with_outputs.append((comment, generated_civil_sentence))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\033[91mGenerated civil alternative-BART_large:\\033[0m\", generated_civil_sentence)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        generated_civil_sentence = Bart_base(comment)\n",
    "            # Add the comment to the list\n",
    "        comments.append(generated_civil_sentence)\n",
    "        # Create a pandas DataFrame from the collected comments\n",
    "        data = {'message': comments, 'label': 0}\n",
    "        df = pd.DataFrame(data)\n",
    "        preds_flat_test = detection(df)\n",
    "\n",
    "        if preds_flat_test[-1]==0:\n",
    "            print(\"\\033[92mGenerated civil alternative-BART_base:\\033[0m\", generated_civil_sentence)\n",
    "            comments_with_outputs.append((comment, generated_civil_sentence))\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"\\033[91mGenerated civil alternative-BART_base:\\033[0m\", generated_civil_sentence)\n",
    "            comments_with_outputs.append((comment, \"Not Generated\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(comments_with_outputs, columns=[\"uncivil\", \"civil\"])\n",
    "output_csv_file = \"civilAlternatives_by_BART_originalGitData.csv\" \n",
    "output_df.to_csv(output_csv_file, index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a206f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_gru",
   "language": "python",
   "name": "toxic_gru"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
