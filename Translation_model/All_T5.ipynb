{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataloader_test):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in dataloader_test:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels':         batch[2],\n",
    "                }  \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        logits = outputs[1]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681be3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(df):\n",
    "    \n",
    "    encoded_test_val = tokenizer.batch_encode_plus(\n",
    "     df.message.values, \n",
    "     add_special_tokens=True, \n",
    "     return_attention_mask=True, \n",
    "     pad_to_max_length=True, \n",
    "     max_length=512, \n",
    "     return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_test = encoded_test_val['input_ids']\n",
    "    attention_masks_test = encoded_test_val['attention_mask']\n",
    "    labels_test = torch.tensor(df.label.values)\n",
    "\n",
    "    dataset_test = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "    batch_size = 8\n",
    "    dataloader_test = DataLoader(dataset_test, \n",
    "                                    sampler=SequentialSampler(dataset_test), \n",
    "                                    batch_size=batch_size)\n",
    "\n",
    "\n",
    "    pred_test = test_model(dataloader_test) \n",
    "    preds_flat_test = np.argmax(pred_test, axis=1).flatten()\n",
    "    return preds_flat_test\n",
    "#     #print(preds_flat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_small(c):\n",
    "    # T5 model\n",
    "    test_sentence = c\n",
    "    inputs = fine_tuned_tokenizer_t5small.encode_plus(\n",
    "        f\"convert uncivil to civil: {test_sentence}\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Ensure tensors are on the same device as the model (cuda or cpu)\n",
    "    fine_tuned_model_t5small.to(device)\n",
    "\n",
    "    output_ids = fine_tuned_model_t5small.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=1024,  # Adjust the max_length as needed\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    decoded_output = fine_tuned_tokenizer_t5small.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_large(c):\n",
    "    # T5 model\n",
    "    test_sentence = c\n",
    "    inputs = fine_tuned_tokenizer_t5large.encode_plus(\n",
    "        f\"convert uncivil to civil: {test_sentence}\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Ensure tensors are on the same device as the model (cuda or cpu)\n",
    "    fine_tuned_model_t5large.to(device)\n",
    "\n",
    "    output_ids = fine_tuned_model_t5large.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=1024,  # Adjust the max_length as needed\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    decoded_output = fine_tuned_tokenizer_t5large.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_base(c):\n",
    "    # T5 model\n",
    "    test_sentence = c\n",
    "    inputs = fine_tuned_tokenizer_t5base.encode_plus(\n",
    "        f\"convert uncivil to civil: {test_sentence}\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Ensure tensors are on the same device as the model (cuda or cpu)\n",
    "    fine_tuned_model_t5base.to(device)\n",
    "\n",
    "    output_ids = fine_tuned_model_t5base.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=1024,  # Adjust the max_length as needed\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    decoded_output = fine_tuned_tokenizer_t5base.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set the logging level for the transformers module\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Suppress specific FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `pad_to_max_length` argument is deprecated*\")\n",
    "\n",
    "# Initialize an empty list to store comments\n",
    "comments = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#t5-small model\n",
    "fine_tuned_model_t5small = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5_small\")\n",
    "fine_tuned_tokenizer_t5small = T5Tokenizer.from_pretrained(\"fine_tuned_t5_small\")\n",
    "\n",
    "#t5-base model\n",
    "fine_tuned_model_t5base = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5_base\")\n",
    "fine_tuned_tokenizer_t5base = T5Tokenizer.from_pretrained(\"fine_tuned_t5_base\")\n",
    "\n",
    "#t5-large model\n",
    "fine_tuned_model_t5large = T5ForConditionalGeneration.from_pretrained(\"fine_tuned_t5_large\")\n",
    "fine_tuned_tokenizer_t5large = T5Tokenizer.from_pretrained(\"fine_tuned_t5_large\")\n",
    "\n",
    "\n",
    "input_csv_file = \"/home/mdr614/Toxic_GRU/Fresh/random2000_for_sentimentAnalysis.csv\"  # Replace with your CSV file path\n",
    "comments_df = pd.read_csv(input_csv_file, header=None, names=[\"comment\"])\n",
    "\n",
    "print(comments_df.head)\n",
    "\n",
    "# Create an empty list to store comments and their associated outputs\n",
    "comments_with_outputs = []\n",
    "\n",
    "count=0\n",
    "\n",
    "for inp_sentence in comments_df[\"comment\"]:\n",
    "    # Collect comment from the user or exit the loop\n",
    "    print(inp_sentence)\n",
    "    comment = inp_sentence\n",
    "    \n",
    "    \n",
    "    # Add the comment to the list\n",
    "    comments.append(comment)\n",
    "    \n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "#     \n",
    "    decoded_output = t5_large(comment)\n",
    "    # Add the comment to the list\n",
    "    comments.append(decoded_output)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "    if preds_flat_test[-1]==0:\n",
    "        print(\"\\033[92mGenerated civil alternative-t5largel:\\033[0m\", decoded_output)\n",
    "        comments_with_outputs.append((comment, decoded_output))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\033[91mGenerated civil alternative-t5large:\\033[0m\", decoded_output)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        decoded_output = t5_base(comment)\n",
    "        # Add the comment to the list\n",
    "        comments.append(decoded_output)\n",
    "        # Create a pandas DataFrame from the collected comments\n",
    "        data = {'message': comments, 'label': 0}\n",
    "        df = pd.DataFrame(data)\n",
    "        preds_flat_test = detection(df)\n",
    "\n",
    "        if preds_flat_test[-1]==0:\n",
    "            print(\"\\033[92mGenerated civil alternative-t5base:\\033[0m\", decoded_output)\n",
    "            comments_with_outputs.append((comment, decoded_output))\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"\\033[91mGenerated civil alternative-t5base:\\033[0m\", decoded_output)\n",
    "            print(\"\\n\")\n",
    "            decoded_output = t5_small(comment)\n",
    "            # Add the comment to the list\n",
    "            comments.append(decoded_output)\n",
    "            # Create a pandas DataFrame from the collected comments\n",
    "            data = {'message': comments, 'label': 0}\n",
    "            df = pd.DataFrame(data)\n",
    "            preds_flat_test = detection(df)\n",
    "            if preds_flat_test[-1]==0:\n",
    "                print(\"\\033[92mGenerated civil alternative-t5small:\\033[0m\", decoded_output)\n",
    "                comments_with_outputs.append((comment, decoded_output))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"\\033[91mGenerated civil alternative-t5small:\\033[0m\", decoded_output)\n",
    "                comments_with_outputs.append((comment, \"Not Generated\"))\n",
    "                print(\"\\n\")\n",
    "    count+=1\n",
    "    print(\"count:\",count)           \n",
    "                \n",
    "output_df = pd.DataFrame(comments_with_outputs, columns=[\"uncivil\", \"civil\"])\n",
    "output_csv_file = \"civilAlternatives_by_T5_random2000.csv\" \n",
    "output_df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b22195",
   "metadata": {},
   "source": [
    "## MarianMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set the logging level for the transformers module\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Suppress specific FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `pad_to_max_length` argument is deprecated*\")\n",
    "\n",
    "# Initialize an empty list to store comments\n",
    "comments = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# Load the fine-tuned MarianMT model\n",
    "fine_tuned_marian_model = MarianMTModel.from_pretrained(\"marian_output\")\n",
    "tokenizer_Marian = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ROMANCE\")\n",
    "\n",
    "input_csv_file = \"/home/mdr614/Toxic_GRU/Fresh/random2000_marian.csv\"  # Replace with your CSV file path\n",
    "comments_df = pd.read_csv(input_csv_file, header=None, names=[\"comment\"])\n",
    "\n",
    "# Create an empty list to store comments and their associated outputs\n",
    "comments_with_outputs = []\n",
    "\n",
    "count=0\n",
    "for inp_sentence in comments_df[\"comment\"]:\n",
    "    # Collect comment from the user or exit the loop\n",
    "    comment = inp_sentence\n",
    "\n",
    "    # Add the comment to the list\n",
    "    comments.append(comment)\n",
    "\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    #for MarianMT\n",
    "    uncivil_sentence = comment\n",
    "    inputs = tokenizer_Marian(uncivil_sentence, return_tensors=\"pt\", max_length=1024, trancation=True)\n",
    "    generated_ids = fine_tuned_marian_model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_Marian.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    subword_units = generated_civil_sentence.split('‚ñÅ')\n",
    "    # Remove empty strings resulting from the split\n",
    "    subword_units = [unit for unit in subword_units if unit]\n",
    "    # Join subword units into words\n",
    "    cleaned_sentence = ' '.join(subword_units)\n",
    "\n",
    "                    # Add the comment to the list\n",
    "    comments.append(cleaned_sentence)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "    if preds_flat_test[-1]==0:\n",
    "        print(\"\\033[92mGenerated civil alternative-MarianMT:\\033[0m\", cleaned_sentence)\n",
    "        comments_with_outputs.append((comment, cleaned_sentence))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\033[91mGenerated civil alternative-MarianMT:\\033[0m\", cleaned_sentence)\n",
    "        comments_with_outputs.append((comment, \"Not Generated\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    count+=1\n",
    "    print(\"count:\", count)\n",
    "\n",
    "output_df = pd.DataFrame(comments_with_outputs, columns=[\"uncivil\", \"civil\"])\n",
    "output_csv_file = \"civilAlternatives_by_MarianMT_random2000.csv\"\n",
    "output_df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b23aa4",
   "metadata": {},
   "source": [
    "## NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f83e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set the logging level for the transformers module\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Suppress specific FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `pad_to_max_length` argument is deprecated*\")\n",
    "\n",
    "# Initialize an empty list to store comments\n",
    "comments = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "#Load the fine_tuned Nllb model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "fine_tuned_nllb_model = AutoModelForSeq2SeqLM.from_pretrained(\"nllb-200-distilled-600M_up\")\n",
    "tokenizer_nllb = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "\n",
    "\n",
    "input_csv_file = \"/home/mdr614/Toxic_GRU/Fresh/random2000_for_nllb.csv\"  # Replace with your CSV file path\n",
    "comments_df = pd.read_csv(input_csv_file, header=None, names=[\"comment\"])\n",
    "\n",
    "# Create an empty list to store comments and their associated outputs\n",
    "comments_with_outputs = []\n",
    "\n",
    "count=0\n",
    "\n",
    "\n",
    "for inp_sentence in comments_df[\"comment\"]:\n",
    "    # Collect comment from the user or exit the loop\n",
    "    comment = inp_sentence\n",
    "\n",
    "    # Add the comment to the list\n",
    "    comments.append(comment)\n",
    "\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    #for nllb\n",
    "    uncivil_sentence = comment\n",
    "    inputs = tokenizer_nllb(uncivil_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    generated_ids = fine_tuned_nllb_model.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_nllb.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Add the comment to the list\n",
    "    comments.append(generated_civil_sentence)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "    if preds_flat_test[-1]==0:\n",
    "        print(\"\\033[92mGenerated civil alternative-Nllb:\\033[0m\", generated_civil_sentence)\n",
    "        comments_with_outputs.append((comment, generated_civil_sentence))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\033[91mGenerated civil alternative-Nllb:\\033[0m\", generated_civil_sentence)\n",
    "        comments_with_outputs.append((comment, \"Not Generated\"))\n",
    "        print(\"\\n\")\n",
    "    count+=1\n",
    "    print(\"count:\",count)\n",
    "    if count==700:\n",
    "        break\n",
    "\n",
    "output_df = pd.DataFrame(comments_with_outputs, columns=[\"uncivil\", \"civil\"])\n",
    "output_csv_file = \"civilAlternatives_by_NLLB_originalGitData.csv\"\n",
    "output_df.to_csv(output_csv_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e1ec8",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bart_large(c):\n",
    "    uncivil_sentence = c\n",
    "    inputs = tokenizer_Bart_large(uncivil_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    generated_ids = fine_tuned_bart_large.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_Bart_large.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_civil_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bart_base(c):\n",
    "    uncivil_sentence = c\n",
    "    inputs = tokenizer_Bart_base(uncivil_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    generated_ids = fine_tuned_bart_base.generate(**inputs, max_new_tokens=1024)\n",
    "    generated_civil_sentence = tokenizer_Bart_base.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_civil_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ff081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set the logging level for the transformers module\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Suppress specific FutureWarning messages\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The `pad_to_max_length` argument is deprecated*\")\n",
    "\n",
    "# Initialize an empty list to store comments\n",
    "comments = []\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                          do_lower_case=True)\n",
    "label_dict = {0: 0, 1: 1}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the fine-tuned BART_large model\n",
    "fine_tuned_bart_large = BartForConditionalGeneration.from_pretrained(\"fine_tuned_bart_large\")\n",
    "tokenizer_Bart_large = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Load the fine-tuned BART_large model\n",
    "fine_tuned_bart_base = BartForConditionalGeneration.from_pretrained(\"fine_tuned_bart_base\")\n",
    "tokenizer_Bart_base = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "\n",
    "\n",
    "input_csv_file = \"/home/mdr614/Toxic_GRU/Fresh/random2000_for_bart.csv\"  # Replace with your CSV file path\n",
    "comments_df = pd.read_csv(input_csv_file, header=None, names=[\"comment\"])\n",
    "\n",
    "# Create an empty list to store comments and their associated outputs\n",
    "comments_with_outputs = []\n",
    "\n",
    "count=0\n",
    "\n",
    "for inp_sentence in comments_df[\"comment\"]:\n",
    "    # Collect comment from the user or exit the loop\n",
    "    comment = inp_sentence\n",
    "\n",
    "\n",
    "    # Add the comment to the list\n",
    "    comments.append(comment)\n",
    "\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    generated_civil_sentence = Bart_large(comment)\n",
    "            # Add the comment to the list\n",
    "    comments.append(generated_civil_sentence)\n",
    "    # Create a pandas DataFrame from the collected comments\n",
    "    data = {'message': comments, 'label': 0}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    preds_flat_test = detection(df)\n",
    "\n",
    "    if preds_flat_test[-1]==0:\n",
    "        print(\"\\033[92mGenerated civil alternative-BART_large:\\033[0m\", generated_civil_sentence)\n",
    "        comments_with_outputs.append((comment, generated_civil_sentence))\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"\\033[91mGenerated civil alternative-BART_large:\\033[0m\", generated_civil_sentence)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        generated_civil_sentence = Bart_base(comment)\n",
    "            # Add the comment to the list\n",
    "        comments.append(generated_civil_sentence)\n",
    "        # Create a pandas DataFrame from the collected comments\n",
    "        data = {'message': comments, 'label': 0}\n",
    "        df = pd.DataFrame(data)\n",
    "        preds_flat_test = detection(df)\n",
    "\n",
    "        if preds_flat_test[-1]==0:\n",
    "            print(\"\\033[92mGenerated civil alternative-BART_base:\\033[0m\", generated_civil_sentence)\n",
    "            comments_with_outputs.append((comment, generated_civil_sentence))\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"\\033[91mGenerated civil alternative-BART_base:\\033[0m\", generated_civil_sentence)\n",
    "            comments_with_outputs.append((comment, \"Not Generated\"))\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    count+=1\n",
    "    print(\"Count: \",count)\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(comments_with_outputs, columns=[\"uncivil\", \"civil\"])\n",
    "output_csv_file = \"civilAlternatives_by_BART_random2000.csv\"\n",
    "output_df.to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_gru",
   "language": "python",
   "name": "toxic_gru"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
